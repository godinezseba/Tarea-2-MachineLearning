{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://www.exalumnos.usm.cl/wp-content/uploads/2015/06/Isotipo-Negro.gif\" title=\"Title text\" width=\"20%\" height=\"20%\" />\n",
    "\n",
    "\n",
    "<hr style=\"height:2px;border:none\"/>\n",
    "<h1 align='center'> INF-393 Máquinas de Aprendizaje II-2019 </h1>\n",
    "\n",
    "<H3 align='center'> Tarea 2 - Fronteras no Lineales </H3>\n",
    "<H3 align='center'> Sección 2 - Problema de Múltiples Anotaciones </H3>\n",
    "<hr style=\"height:2px;border:none\"/>\n",
    "<center>\n",
    "    <h4> Sebastián Godínez San Martín, 201673520-8</h4>\n",
    "    <h4> Daniel Toro, 201673595-K </h4> \n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El aprendizaje supervisado visto en clases da cuenta que existe una única posible etiqueta $z$ (*ground truth*) asociada a un dato $x$ para poder aprender de los datos. Sin embargo, la definición del *ground truth* en algunos casos puede resultar bastante difícil de definir o bastante costoso, por ejemplo en problemas médicos donde se deben realizar invasivos exámenes para conocer la \"verdad absoluta\". Como alternativa se pueden recolectar múltiples anotaciones desde personas inexpertas en el área para poder estimar el *ground truth*. \n",
    "\n",
    "<img src=\"http://www.irishenvironment.com/wp-content/uploads/2013/11/crowdsourcing.jpg\" title=\"Title text\" width=\"40%\" />\n",
    "\n",
    "En esta actividad se trabajará en el caso en que contamos con múltiples anotaciones por cada dato de entrada $x_i$ dadas por personas inexpertas a través de Amazon Mechanical Turk (__[AMT](https://www.mturk.com/)__), es decir, $y^{(1)}, y^{(2)} \\ldots y^{(T_i)}$. El problema de predicción trabajado será el mismo de la actividad anterior, el análisis de sentimiento de un extracto de texto en Rotten Tomatoes. Los datos trabajados [[8]](#refs) pueden ser descargados de la página del autor.\n",
    "```\n",
    "wget http://fprodrigues.com//mturk-datasets.tar.gz\n",
    "```\n",
    "\n",
    "De esta manera trabajaremos con el archivo *mturk_answers.csv* en la carpeta *sentiment polarity* que se puede cargar con pandas. En este archivo se tendrán múltiples filas/registros por cada dato de entrenamiento, cada uno representando la etiqueta que entregó una persona (*worker*) a ese dato, los detalles de columnas son:\n",
    "* *WorkerId*: identificador de la persona que etiquetó/anotó el dato\n",
    "* *Input.id*: identificador del dato a etiquetar/anotar\n",
    "* *Input.original_sentence*: texto original del dato\n",
    "* *Input.stemmed_sent*: texto pre-procesado\n",
    "* *Input.true_sent*: sentimiento real (*ground truth*) del dato\n",
    "* *Answer.sent*:   etiqueta/anotación que entregó la persona\n",
    "```python\n",
    "df = pd.read_csv(\"./mturk-datasets/sentiment_polarity/mturk_answers.csv\") \n",
    "```\n",
    "\n",
    "> El objetivo de la actividad será el de obtener un modelo predictor del *ground truth* sin entrenar directamente con esto, sino que utilizar las múltiples anotaciones de las personas\n",
    "\n",
    "### Importante\n",
    "* Deberá crear un conjunto aleatorio de pruebas que solo contenga los textos y el sentimiento *ground truth* (sin repeticiones)\n",
    "* Los valores de *ground truth* están **solo para evaluar**, no puede utilizarlos para entrenar o tomar decisiones en su modelo.\n",
    "* La métrica de evaluación será el *accuracy score*\n",
    "```python\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y_true, y_pred)\n",
    "```\n",
    "\n",
    "#### Sugerencias\n",
    "Para representar los textos, a diferencia de lo visto en la sección 1, se puede utilizar lo que son la representación vectorial de palabras (*word vectors*), a través de modelos especializados entrenados para aprender una representación en que palabras similares estén cercanas en el espacio vectorial. Puede utilizar los vectores livianos de __[GLOVE](https://nlp.stanford.edu/projects/glove/)__ en su versión entradas en textos de Wikipedia con 6 billones de palabras/tokens.\n",
    "```python\n",
    "EMBEDDING_DIM = 300\n",
    "GLOVE_FILE = \"./glove.6B/glove.6B.%dd.txt\"%(EMBEDDING_DIM)\n",
    "embeddings_index = {}\n",
    "with open(GLOVE_FILE) as file:\n",
    "    for line in file:\n",
    "        values = line.split()\n",
    "        embeddings_index[values[0]] = np.asarray(values[1:], dtype='float32')\n",
    "...\n",
    "embeddings_index.get(word)\n",
    "```\n",
    "> Para descargar GLOVE\n",
    "```\n",
    "wget http://nlp.stanford.edu/data/glove.6B.zip\n",
    "```\n",
    "### Consideraciones\n",
    "Se asume que _mturk-datasets.tar.gz_ y _glove.6B.zip_ estan descomprimidos en la misma carpeta donde se encuentra este Notebook, generando automaticamente las carpetas _mturk-datasets_ y _glove.6B_ respectivamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>WorkerId</th>\n",
       "      <th>Input.id</th>\n",
       "      <th>Input.original_sentence</th>\n",
       "      <th>Input.stemmed_sent</th>\n",
       "      <th>Input.true_sent</th>\n",
       "      <th>Answer.sent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A2HD5XMM48KKJW</td>\n",
       "      <td>4518</td>\n",
       "      <td>the cast is phenomenal , especially the women .</td>\n",
       "      <td>cast phenomen especi women</td>\n",
       "      <td>pos</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A2HD5XMM48KKJW</td>\n",
       "      <td>10415</td>\n",
       "      <td>the metaphors are provocative , but too often ...</td>\n",
       "      <td>metaphor provoc often viewer left puzzl mechan...</td>\n",
       "      <td>neg</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A2HD5XMM48KKJW</td>\n",
       "      <td>7098</td>\n",
       "      <td>while there's something intrinsically funny ab...</td>\n",
       "      <td>there someth intrins funni sir anthoni hopkin ...</td>\n",
       "      <td>neg</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A2HD5XMM48KKJW</td>\n",
       "      <td>4396</td>\n",
       "      <td>a harrowing account of a psychological breakdo...</td>\n",
       "      <td>harrow account psycholog breakdown</td>\n",
       "      <td>pos</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A2HD5XMM48KKJW</td>\n",
       "      <td>2812</td>\n",
       "      <td>. . . a visually seductive , unrepentantly tr...</td>\n",
       "      <td>visual seduct unrepentantli trashi rice instal...</td>\n",
       "      <td>pos</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27741</th>\n",
       "      <td>A4F3BK6J0PCVH</td>\n",
       "      <td>7144</td>\n",
       "      <td>the movie fails to live up to the sum of its p...</td>\n",
       "      <td>movi fail live sum part</td>\n",
       "      <td>neg</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27742</th>\n",
       "      <td>A4F3BK6J0PCVH</td>\n",
       "      <td>8293</td>\n",
       "      <td>the balkans provide the obstacle course for th...</td>\n",
       "      <td>balkan provid obstacl cours love good woman</td>\n",
       "      <td>neg</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27743</th>\n",
       "      <td>A4F3BK6J0PCVH</td>\n",
       "      <td>2929</td>\n",
       "      <td>this is a smart movie that knows its classical...</td>\n",
       "      <td>smart movi know classic music know freud know ...</td>\n",
       "      <td>pos</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27744</th>\n",
       "      <td>A4F3BK6J0PCVH</td>\n",
       "      <td>6902</td>\n",
       "      <td>there's something with potential here , but th...</td>\n",
       "      <td>there someth potenti movi decid lavinia conser...</td>\n",
       "      <td>neg</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27745</th>\n",
       "      <td>A4F3BK6J0PCVH</td>\n",
       "      <td>1369</td>\n",
       "      <td>. . . a fun little timewaster , helped especi...</td>\n",
       "      <td>fun littl timewast help especi cool presenc je...</td>\n",
       "      <td>pos</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>27746 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             WorkerId  Input.id  \\\n",
       "0      A2HD5XMM48KKJW      4518   \n",
       "1      A2HD5XMM48KKJW     10415   \n",
       "2      A2HD5XMM48KKJW      7098   \n",
       "3      A2HD5XMM48KKJW      4396   \n",
       "4      A2HD5XMM48KKJW      2812   \n",
       "...               ...       ...   \n",
       "27741   A4F3BK6J0PCVH      7144   \n",
       "27742   A4F3BK6J0PCVH      8293   \n",
       "27743   A4F3BK6J0PCVH      2929   \n",
       "27744   A4F3BK6J0PCVH      6902   \n",
       "27745   A4F3BK6J0PCVH      1369   \n",
       "\n",
       "                                 Input.original_sentence  \\\n",
       "0       the cast is phenomenal , especially the women .    \n",
       "1      the metaphors are provocative , but too often ...   \n",
       "2      while there's something intrinsically funny ab...   \n",
       "3      a harrowing account of a psychological breakdo...   \n",
       "4       . . . a visually seductive , unrepentantly tr...   \n",
       "...                                                  ...   \n",
       "27741  the movie fails to live up to the sum of its p...   \n",
       "27742  the balkans provide the obstacle course for th...   \n",
       "27743  this is a smart movie that knows its classical...   \n",
       "27744  there's something with potential here , but th...   \n",
       "27745   . . . a fun little timewaster , helped especi...   \n",
       "\n",
       "                                      Input.stemmed_sent Input.true_sent  \\\n",
       "0                             cast phenomen especi women             pos   \n",
       "1      metaphor provoc often viewer left puzzl mechan...             neg   \n",
       "2      there someth intrins funni sir anthoni hopkin ...             neg   \n",
       "3                     harrow account psycholog breakdown             pos   \n",
       "4      visual seduct unrepentantli trashi rice instal...             pos   \n",
       "...                                                  ...             ...   \n",
       "27741                            movi fail live sum part             neg   \n",
       "27742        balkan provid obstacl cours love good woman             neg   \n",
       "27743  smart movi know classic music know freud know ...             pos   \n",
       "27744  there someth potenti movi decid lavinia conser...             neg   \n",
       "27745  fun littl timewast help especi cool presenc je...             pos   \n",
       "\n",
       "      Answer.sent  \n",
       "0             pos  \n",
       "1             neg  \n",
       "2             pos  \n",
       "3             neg  \n",
       "4             pos  \n",
       "...           ...  \n",
       "27741         neg  \n",
       "27742         neg  \n",
       "27743         pos  \n",
       "27744         neg  \n",
       "27745         pos  \n",
       "\n",
       "[27746 rows x 6 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"./mturk-datasets/sentiment_polarity/mturk_answers.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separamos el ground truth del csv y la anotacion de las personas\n",
    "labels_train = df[\"Answer.sent\"] # \"y\" usado para entrenar\n",
    "labels_true = df[\"Input.true_sent\"] # \"y\" real\n",
    "\n",
    "# se transforman a numero\n",
    "# 0 = neg ; 1 = pos\n",
    "def convert(string):\n",
    "    if \"pos\" == string:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "labels_person = [convert(s) for s in labels_train]\n",
    "labels_true = [convert(s) for s in labels_true]\n",
    "\n",
    "# separamos las columnas originales de las filtradas\n",
    "text_original = df[\"Input.original_sentence\"]\n",
    "text = df[\"Input.stemmed_sent\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamaño conjunto entrenamiento: (19423,) \n",
      "Tamaño conjunto validacion: (8323,)\n"
     ]
    }
   ],
   "source": [
    "# separamos en conjunto de pruebas y conjunto de validacion\n",
    "from sklearn.model_selection import train_test_split\n",
    "text_train, text_val, labels_train, labels_val  = train_test_split(text,\n",
    "                                                                   labels_person, test_size= int(text.shape[0]*0.3),\n",
    "                                                                   random_state=0)\n",
    "\n",
    "print(f\"Tamaño conjunto entrenamiento: {text_train.shape} \\nTamaño conjunto validacion: {text_val.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorizamos usando TfidVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf_model = TfidfVectorizer(binary=False, \n",
    "                              ngram_range=(1, 1), \n",
    "                              max_df=1.0, \n",
    "                              min_df=1, \n",
    "                              max_features=None, \n",
    "                              norm='l2', \n",
    "                              use_idf=True, \n",
    "                              sublinear_tf=False)\n",
    "tfidf_model.fit(text_train)\n",
    "features_train_tfid = tfidf_model.transform(text_train)\n",
    "features_val_tfid = tfidf_model.transform(text_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorizamos usando GLOVE\n",
    "EMBEDDING_DIM = 300\n",
    "GLOVE_FILE = \"./glove.6B/glove.6B.%dd.txt\"%(EMBEDDING_DIM)\n",
    "embeddings_index = {}\n",
    "with open(GLOVE_FILE) as file:\n",
    "    for line in file:\n",
    "        values = line.split()\n",
    "        embeddings_index[values[0]] = np.asarray(values[1:], dtype='float32')\n",
    "#embeddings_index.get(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"refs\"></a>\n",
    "## Referencias\n",
    "[1] Keras: Deep Learning library for Theano and TensorFlow. https://keras.io/  \n",
    "[2] https://www.kaggle.com/c/sentiment-analysis-on-movie-reviews  \n",
    "[3] https://en.wikipedia.org/wiki/Stopwords  \n",
    "[4] https://en.wikipedia.org/wiki/Lemmatisation  \n",
    "[5] Landauer, T. K., Foltz, P. W., & Laham, D. (1998). *An introduction to latent semantic analysis*. Discourse processes, 25(2-3), 259-284.  \n",
    "[6] https://github.com/cjhutto/vaderSentiment  \n",
    "[7] https://en.wikipedia.org/wiki/Stemming  \n",
    "[8] Rodrigues, F., Pereira, F., & Ribeiro, B. (2013). *Learning from multiple annotators: distinguishing good from random labelers*. Pattern Recognition Letters, 34(12), 1428-1436."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
